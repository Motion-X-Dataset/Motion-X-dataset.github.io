<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset.">
  <meta name="keywords" content="SMPLX, Diffusion, Character Animation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h4 class="title conference-title is-4">NeurIPS 2023</h4> -->
            <h2 class="title is-2 publication-title">
              Motion-X: A Large-scale 3D Expressive <br> Whole-body Human Motion Dataset
            </h2>
            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=SvaU2GMAAAAJ&hl=zh-CN">Jing Lin</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://ailingzeng.site/">Ailing Zeng</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.sigs.tsinghua.edu.cn/whq_en/main.htm">Haoqian Wang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.leizhang.org/">Lei Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://yu-li.github.io/">Yu Li</a><sup>1</sup>
              </span> -->
              <span class="author-block"></span>
                <a>Anonymous Authors</a>
              </span>

            </div>

            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>International Digital Economy Academy (IDEA),</span>
              <span class="author-block"><sup>2</sup>Shenzhen International Graduate School, Tsinghua University</span>
            </div> -->


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                <a href="http://arxiv.org/abs/2303.16160" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
                <!-- <span class="link-block">
                  <a href="http://arxiv.org/abs/2303.16160" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
                <!-- Paper Link. -->
                <span class="link-block">
                <a href="https://openreview.net/attachment?id=WtajAo0JWU&name=pdf" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://docs.google.com/forms/d/e/1FAIpQLSeb1DwnzGPxXWWjXr8cLFPAYd3ZHlWUtRDAzYoGvAKmS4uBlA/viewform" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Data Access</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://docs.google.com/document/d/1ZMFQxK2RG2VcYgvBZgNhyKDbJ1GzsO4dnOqjeWED7GU/edit?usp=sharing" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Data Access (For Reviewer)</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://docs.google.com/document/d/1xeNQkkxD39Yi6pAtJrFS1UcZ2LyJ6RBwxicwQ2j3-Vs/edit?usp=sharing" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>License</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h5 class="subtitle is-5">
        <b>Motion-X</b> is a large-scale 3D expressive whole-body motion dataset, which comprises 13.7M precise 3D whole-body pose annotations (i.e., SMPL-X) 
        covering 96K motion sequences from massive scenes, meanwhile providing corresponding semantic labels and pose descriptions.

        </h5>
        <img src="./static/images/teaser.png" autoplay muted loop playsinline height="100%">
        <h2 class="subtitle has-text-centered">
          Figure 1: Different from (a) previous motion dataset (e.g., HumanML3D), 
          (b) our dataset captures body, facial expressions, and hand gestures. 
          We highlight the comparisons of facial expressions and hand gestures.
        </h2>
        <img src="./static/images/comp_data.png" autoplay muted loop playsinline height="100%">
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered"></h3>
        <!-- <h3 class="title is-3 has-text-centered">UBody Dataset</h3> -->
        <p align="middle">
          
          <video id="myVideo" src="./static/videos/v3.mp4" controls></video>

        </p>

      </div>
    </div>
  </section>
  
  <p style="margin-bottom: -1.5cm;"></p>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We propose <b>Motion-X</b>, a large-scale 3D expressive whole-body motion dataset. 

      
            </p>
            <p>
              Existing motion datasets predominantly contain body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions. 
              Moreover, they are primarily collected from limited laboratory scenes with textual descriptions manually labeled, limiting their scalability.
              To overcome these limitations, we develop a whole-body motion and text annotation pipeline, which can automatically annotate motion from either single- or multi-view videos 
              and provide comprehensive semantic labels for each video and fine-grained whole-body pose descriptions for each frame. 
              This pipeline is of high precision, cost-effective, and scalable for further research. 
            </p>
            <p>
              Based on it, we construct <b>Motion-X</b>, which comprises <b>13.7M</b> precise 3D whole-body <b>pose annotations</b> (i.e., SMPL-X) covering <b>96K</b> motion sequences from massive scenes. 
              Besides, Motion-X provides <b>13.7M frame-level</b> whole-body pose descriptions and <b>96K sequence-level</b> semantic labels.
            </p>
            <p>
              Comprehensive experiments demonstrate the accuracy of the annotation pipeline and the significant benefit of Motion-X in enhancing expressive, diverse, and natural motion generation, as well as the 3D whole-body human mesh recovery task.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <p style="margin-bottom: -0.5cm;"></p>

  <!-- <section class="hero">
    <div class="hero-body has-text-centered">
      <div class="container">
        <h3 class="title is-3">Comparisons with Other Methods</h3>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item agora_1">
            <img src="./static/images/agora_1.png" class="interpolation-image"
              alt="Interpolation end reference image." />
          </div>

          <div class="item agora_2">
            <img src="./static/images/agora_2.png" class="interpolation-image"
              alt="Interpolation end reference image." />
          </div>

          <div class="item agora_3">
            <img src="./static/images/agora_3.png" class="interpolation-image"
              alt="Interpolation end reference image." />
          </div>

          <div class="item agora_4">
            <img src="./static/images/agora_4.png" class="interpolation-image"
              alt="Interpolation end reference image." />
          </div>

          <div class="item agora_5">
            <img src="./static/images/agora_5.png" class="interpolation-image"
              alt="Interpolation end reference image." />
          </div>

          <div class="item ehf_1">
            <img src="./static/images/ehf_1.png" class="interpolation-image" alt="Interpolation end reference image." />
          </div>

          <div class="item ehf_2">
            <img src="./static/images/ehf_2.png" class="interpolation-image" alt="Interpolation end reference image." />
          </div>

          <div class="item ehf_3">
            <img src="./static/images/ehf_3.png" class="interpolation-image" alt="Interpolation end reference image." />
          </div>

          <div class="item ehf_4">
            <img src="./static/images/ehf_4.png" class="interpolation-image" alt="Interpolation end reference image." />
          </div>

          <div class="item ubody_1">
            <img src="./static/images/ubody_1.png" class="interpolation-image"
              alt="Interpolation end reference image." />
          </div>

          <div class="item ubody_2">
            <img src="./static/images/ubody_2.png" class="interpolation-image"
              alt="Interpolation end reference image." />
          </div>

          <div class="item ubody_3">
            <img src="./static/images/ubody_3.png" class="interpolation-image"
              alt="Interpolation end reference image." />
          </div>

          <div class="item ubody_4">
            <img src="./static/images/ubody_4.png" class="interpolation-image"
              alt="Interpolation end reference image." />
          </div>

          <div class="item ubody_5">
            <img src="./static/images/ubody_5.png" class="interpolation-image"
              alt="Interpolation end reference image." />
          </div>


        </div>
        <h2 class="subtitle has-text-centered">
          From left to right: 1. Input, 2. ExPose, 3. Hand4Whoe, 4. OSX (Ours)
        </h2>
      </div>
    </div>
  </section> -->

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered">Motion-X Dataset</h3>
        <!-- <h3 class="title is-3 has-text-centered">UBody Dataset</h3> -->
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/overview.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Figure 2: SMPL-X motion samples from the Motion-X dataset. 
        </h2>
        <img src="./static/images/vis_annotation.png" autoplay muted loop playsinline height="100%">
        <h2 class="subtitle has-text-centered">
        Figure 3: Overview of Motion-X. It includes: (a) diverse facial expressions extracted from BAUM, (b) indoor motion with expressive face and hand motions, 
        (c) outdoor motion with diverse and challenging poses, and (d) several motion sequences. 
        Purple SMPL-X is the observed frame, and the others are neighboring poses.
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered">Visualization of motion annotations from massive online videos</h3>
        <!-- <h3 class="title is-3 has-text-centered">UBody Dataset</h3> -->
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/cross_the_single_plank_bridge_.mp4" type="video/mp4">
        </video>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/A_Monster_Powers_Up_Motion_Actorlnc_Sugiguchi_Hideki_clip1.mp4" type="video/mp4">
        </video>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/Aerial_Kick_Kungfu_wushu_turn_over_clip10.mp4" type="video/mp4">
        </video>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/Being_Chased_clip1.mp4" type="video/mp4">
        </video>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/Better_View_clip1.mp4" type="video/mp4">
        </video>
        </video>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/perform_ballet_clip39.mp4" type="video/mp4">
        </video>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/perform_ballet_clip20.mp4" type="video/mp4">
        </video>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/Play_the_stringed_guqin_clip7.mp4" type="video/mp4">
        </video>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/perform_ballet_clip24.mp4" type="video/mp4">
        </video> 
        <br>
          <img src="./static/images/whole_annotation.png" autoplay muted loop playsinline height="100%">
          <h2 class="subtitle has-text-centered">
            Figure 4: llustration of the overall data collection and annotation pipeline.
          </h2
        <br/>
        <br>
          <img src="./static/images/motion_annotation.png" autoplay muted loop playsinline height="100%">
          <h2 class="subtitle has-text-centered">
            Figure 5: The automatic pipeline for the whole-body motion capture from  massive videos, including 2D and 3D whole-body keypoints estimation, 
            local pose optimization, and global translation optimization. This pipeline supports both single- and multi-view inputs. 
            Dashed lines represent the handling of multi-view data exclusively.
          </h2>
        <br/>
        <br>
          <img src="./static/images/text_annotation.png" autoplay muted loop playsinline height="100%">
          <h2 class="subtitle has-text-centered">
            Figure 6: Illustration of (a) annotation of the whole-body pose description, and (b) an example of the text labels.
          </h2>
        <br/>
      
      </div>
    </div>
  </section>
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered">UBody Dataset</h3>
        <h5 class="subtitle is-5">
          UBody is a large-scale Upper-Body dataset with the following annotations: 
          <style>
            ul {
              list-style-type: circle;
            }
          </style>
          <ul>
            <li>2D whole-body keypoints</li>
            <li>3D SMPLX annotations</li>
            <li>frame validity label </li>
            <li>person bbox, hand bbox </li>
          </ul>
        </h5>
        <p align="middle">
          <img src="./static/videos/demo_video.gif" width="720" height="240">
        </p>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/demo_video.gif" type="video/mp4">
        </video> -->
      </div>
    </div>
  </section> -->

  <p style="margin-bottom: -0.5cm;"></p>
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered">Examples of the sub-datasets in Motion-X</h3>
        <br>
          <img src="./static/images/stat_motionx.png" autoplay muted loop playsinline height="100%">
          <h2 class="subtitle has-text-centered">
            Figure 7: Motion-X is a superset consisting of eight public datasets and 15K online videos. 
            We annotate all data via our proposed motion and text annotation methods.
          </h2>
        <br/>
        <!-- <h3 class="title is-3 has-text-centered">UBody Dataset</h3> -->
        <!-- <p align="middle">
          <img src="./static/videos/online_video.mp4" width="720" height="240">
        </p> -->
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/online_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Online Videos
        </h2>

        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/aist.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Multi-View Datasest (AIST++ and NTU-RGBD120)
        </h2>

        <video id="teaser" autoplay muted loop playsinline height="100%" width="870">
          <source src="./static/videos/grab.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Human-scene-interaction Datasets (GRAB and EgoBody)
        </h2>

        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/HAA500.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Action Recognition Datasets (HAA500 and HuMMan)
        </h2>

        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/humanml.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Body-Only Motion Capture Datasets (AMASS)
        </h2>

      </div>
    </div>
  </section>


  <!-- <h2 class="title">Contact Us</h2>
<style>
  ul {
    list-style-type: circle;
  }
</style>
<ul>
  <li>For detailed questions about this work, please contact Jing Lin (jinglin.stu@gmail.com).</li>
  <li>We are looking for talented, motivated, and creative research and engineering interns working on human-centric visual understanding and generation topics. If you are interested, please send your CV to Ailing Zeng (zengailing@idea.edu.cn).</li>
</ul>
    </div>
  </section> -->



  <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
    <source src="./static/videos/demo_video.gif" type="video/mp4">
  </video> -->



  <p style="margin-bottom: -1.5cm;"></p>
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content is-size-6">
  </code></pre>
  <h2 class="title">License</h2>
  <style>
    ul {
      list-style-type: circle;
    }
  </style>
  <ul>
    All data is distributed under the CC BY-NC-SA (Attribution-NonCommercial-ShareAlike) license. For the sub-datasets, although we annotate the motion-text labels with our annoation pipeline, we would ask the user to read the original license of each original dataset, 
      and we would only provide our annotated result to the user with the approvals from the original Institution. 
      Here we provide the link of the used assets:  </li>
    <li> <a href="https://mimoza.marmara.edu.tr/~cigdem.erdem/BAUM1/">BAUM</a>, <a href="https://google.github.io/aistplusplus_dataset/">AIST++</a>, 
      <a href="https://sanweiliti.github.io/egobody/egobody.html">EgoBody</a>  datasets are CC-BY 4.0 licensed. </li>
    <li> <a href="https://www.cse.ust.hk/haa/">HAA500</a> dataset is MIT licensed.  </li>
    <li> <a href="https://caizhongang.github.io/projects/HuMMan/">HuMMan</a> dataset is under S-Lab License v1.0. </li>
    <li> <a href="https://rose1.ntu.edu.sg/dataset/actionRecognition/">NTU-RGBD120</a>, 
      <a href="https://grab.is.tue.mpg.de/">GRAB</a>, <a href="https://amass.is.tue.mpg.de/">AMASS</a> dataset is released for academic research only and is free to researchers from educational or research institutes for non-commercial purposes. </li>
    <li> Other data is under CC BY-SA 4.0 license. </li>

  </ul>
      </div>
    </section>
    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-size-6">
            <div class="content">
              <p>
                This website is created with this <a href="https://nerfies.github.io/">template</a>. 
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

  </body>

  </html>
